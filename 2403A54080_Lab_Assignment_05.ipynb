{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VISHNUVARDHAN2730/NLP-4080/blob/main/2403A54080_Lab_Assignment_05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Using NLTK**"
      ],
      "metadata": {
        "id": "wnnFO1y43Kvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load the dataset**"
      ],
      "metadata": {
        "id": "fMGiueLbLimh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/arxiv_data.csv\", engine='python', nrows=1000)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "IolGl-L3Lonq",
        "outputId": "6c92d108-7d6c-448e-9362-55380f23562f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              titles  \\\n",
              "0  Survey on Semantic Stereo Matching / Semantic ...   \n",
              "1  FUTURE-AI: Guiding Principles and Consensus Re...   \n",
              "2  Enforcing Mutual Consistency of Hard Regions f...   \n",
              "3  Parameter Decoupling Strategy for Semi-supervi...   \n",
              "4  Background-Foreground Segmentation for Interio...   \n",
              "\n",
              "                                           summaries  \\\n",
              "0  Stereo matching is one of the widely used tech...   \n",
              "1  The recent advancements in artificial intellig...   \n",
              "2  In this paper, we proposed a novel mutual cons...   \n",
              "3  Consistency training has proven to be an advan...   \n",
              "4  To ensure safety in automated driving, the cor...   \n",
              "\n",
              "                         terms  \n",
              "0           ['cs.CV', 'cs.LG']  \n",
              "1  ['cs.CV', 'cs.AI', 'cs.LG']  \n",
              "2           ['cs.CV', 'cs.AI']  \n",
              "3                    ['cs.CV']  \n",
              "4           ['cs.CV', 'cs.LG']  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ba9f5e1c-6320-495e-8839-ff15a4fd6bdb\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>titles</th>\n",
              "      <th>summaries</th>\n",
              "      <th>terms</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Survey on Semantic Stereo Matching / Semantic ...</td>\n",
              "      <td>Stereo matching is one of the widely used tech...</td>\n",
              "      <td>['cs.CV', 'cs.LG']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>FUTURE-AI: Guiding Principles and Consensus Re...</td>\n",
              "      <td>The recent advancements in artificial intellig...</td>\n",
              "      <td>['cs.CV', 'cs.AI', 'cs.LG']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Enforcing Mutual Consistency of Hard Regions f...</td>\n",
              "      <td>In this paper, we proposed a novel mutual cons...</td>\n",
              "      <td>['cs.CV', 'cs.AI']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Parameter Decoupling Strategy for Semi-supervi...</td>\n",
              "      <td>Consistency training has proven to be an advan...</td>\n",
              "      <td>['cs.CV']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Background-Foreground Segmentation for Interio...</td>\n",
              "      <td>To ensure safety in automated driving, the cor...</td>\n",
              "      <td>['cs.CV', 'cs.LG']</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ba9f5e1c-6320-495e-8839-ff15a4fd6bdb')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ba9f5e1c-6320-495e-8839-ff15a4fd6bdb button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ba9f5e1c-6320-495e-8839-ff15a4fd6bdb');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 1000,\n  \"fields\": [\n    {\n      \"column\": \"titles\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"PI-RCNN: An Efficient Multi-sensor 3D Object Detector with Point-based Attentive Cont-conv Fusion Module\",\n          \"Studying the Plasticity in Deep Convolutional Neural Networks using Random Pruning\",\n          \"A Gentle Introduction to Deep Learning in Medical Image Processing\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summaries\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"LIDAR point clouds and RGB-images are both extremely essential for 3D object\\ndetection. So many state-of-the-art 3D detection algorithms dedicate in fusing\\nthese two types of data effectively. However, their fusion methods based on\\nBirds Eye View (BEV) or voxel format are not accurate. In this paper, we\\npropose a novel fusion approach named Point-based Attentive Cont-conv\\nFusion(PACF) module, which fuses multi-sensor features directly on 3D points.\\nExcept for continuous convolution, we additionally add a Point-Pooling and an\\nAttentive Aggregation to make the fused features more expressive. Moreover,\\nbased on the PACF module, we propose a 3D multi-sensor multi-task network\\ncalled Pointcloud-Image RCNN(PI-RCNN as brief), which handles the image\\nsegmentation and 3D object detection tasks. PI-RCNN employs a segmentation\\nsub-network to extract full-resolution semantic feature maps from images and\\nthen fuses the multi-sensor features via powerful PACF module. Beneficial from\\nthe effectiveness of the PACF module and the expressive semantic features from\\nthe segmentation module, PI-RCNN can improve much in 3D object detection. We\\ndemonstrate the effectiveness of the PACF module and PI-RCNN on the KITTI 3D\\nDetection benchmark, and our method can achieve state-of-the-art on the metric\\nof 3D AP.\",\n          \"Recently there has been a lot of work on pruning filters from deep\\nconvolutional neural networks (CNNs) with the intention of reducing\\ncomputations.The key idea is to rank the filters based on a certain criterion\\n(say, l1-norm) and retain only the top ranked filters. Once the low scoring\\nfilters are pruned away the remainder of the network is fine tuned and is shown\\nto give performance comparable to the original unpruned network. In this work,\\nwe report experiments which suggest that the comparable performance of the\\npruned network is not due to the specific criterion chosen but due to the\\ninherent plasticity of deep neural networks which allows them to recover from\\nthe loss of pruned filters once the rest of the filters are fine-tuned.\\nSpecifically we show counter-intuitive results wherein by randomly pruning\\n25-50% filters from deep CNNs we are able to obtain the same performance as\\nobtained by using state-of-the-art pruning methods. We empirically validate our\\nclaims by doing an exhaustive evaluation with VGG-16 and ResNet-50. We also\\nevaluate a real world scenario where a CNN trained on all 1000 ImageNet classes\\nneeds to be tested on only a small set of classes at test time (say, only\\nanimals). We create a new benchmark dataset from ImageNet to evaluate such\\nclass specific pruning and show that even here a random pruning strategy gives\\nclose to state-of-the-art performance. Unlike existing approaches which mainly\\nfocus on the task of image classification, in this work we also report results\\non object detection and image segmentation. We show that using a simple random\\npruning strategy we can achieve significant speed up in object detection (74%\\nimprovement in fps) while retaining the same accuracy as that of the original\\nFaster RCNN model. Similarly we show that the performance of a pruned\\nSegmentation Network (SegNet) is actually very similar to that of the original\\nunpruned SegNet.\",\n          \"This paper tries to give a gentle introduction to deep learning in medical\\nimage processing, proceeding from theoretical foundations to applications. We\\nfirst discuss general reasons for the popularity of deep learning, including\\nseveral major breakthroughs in computer science. Next, we start reviewing the\\nfundamental basics of the perceptron and neural networks, along with some\\nfundamental theory that is often omitted. Doing so allows us to understand the\\nreasons for the rise of deep learning in many application domains. Obviously\\nmedical image processing is one of these areas which has been largely affected\\nby this rapid progress, in particular in image detection and recognition, image\\nsegmentation, image registration, and computer-aided diagnosis. There are also\\nrecent trends in physical simulation, modelling, and reconstruction that have\\nled to astonishing results. Yet, some of these approaches neglect prior\\nknowledge and hence bear the risk of producing implausible results. These\\napparent weaknesses highlight current limitations of deep learning. However, we\\nalso briefly discuss promising approaches that might be able to resolve these\\nproblems in the future.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"terms\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 133,\n        \"samples\": [\n          \"['cs.CV', 'eess.IV', 'stat.AP', '62P99']\",\n          \"['cs.CV', 'cs.LG', 'I.2.1, I.4.6,']\",\n          \"['cs.CV', 'cs.CR', 'cs.LG']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Clean the data**"
      ],
      "metadata": {
        "id": "POWxx7JkL1TI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_data(data):\n",
        "    data = re.sub(r'http\\S+|www\\S+|https\\S+', '', data, flags=re.MULTILINE)\n",
        "    data = re.sub(r'<.*?>', '', data)\n",
        "    data = re.sub(r'@\\w+', '', data)\n",
        "    data = re.sub(r'#\\w+', '', data)\n",
        "    data = data.lower()\n",
        "    emoji_pattern = re.compile(\n",
        "        \"[\"\n",
        "        \"\\U0001F600-\\U0001F64F\"\n",
        "        \"\\U0001F300-\\U0001F5FF\"\n",
        "        \"\\U0001F680-\\U0001F6FF\"\n",
        "        \"\\U0001F1E0-\\U0001F1FF\"\n",
        "        \"\\U00002702-\\U000027B0\"\n",
        "        \"\\U000024C2-\\U0001F251\"\n",
        "        \"]+\", flags=re.UNICODE\n",
        "    )\n",
        "    data = emoji_pattern.sub(r'', data)\n",
        "    data = re.sub(r'[^a-zA-Z0-9\\s]', '', data)\n",
        "    data = re.sub(r'\\s+', ' ', data).strip()\n",
        "    return data"
      ],
      "metadata": {
        "id": "8Zcn8SZlL5zA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['processed_summaries'] = df['summaries'].apply(clean_data)\n",
        "print(df[['summaries', 'processed_summaries']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4j9L6E0CMuje",
        "outputId": "3b33e6de-8616-4059-e054-dc41cbef736d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                           summaries  \\\n",
            "0  Stereo matching is one of the widely used tech...   \n",
            "1  The recent advancements in artificial intellig...   \n",
            "2  In this paper, we proposed a novel mutual cons...   \n",
            "3  Consistency training has proven to be an advan...   \n",
            "4  To ensure safety in automated driving, the cor...   \n",
            "\n",
            "                                 processed_summaries  \n",
            "0  stereo matching is one of the widely used tech...  \n",
            "1  the recent advancements in artificial intellig...  \n",
            "2  in this paper we proposed a novel mutual consi...  \n",
            "3  consistency training has proven to be an advan...  \n",
            "4  to ensure safety in automated driving the corr...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Word Tokenization**"
      ],
      "metadata": {
        "id": "-9Zwr2XbM4fV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "df['tokenized_summaries'] = df['processed_summaries'].apply(word_tokenize)\n",
        "print(df[['processed_summaries', 'tokenized_summaries']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdmCwAo9M70M",
        "outputId": "39f74b66-3a96-4746-b90f-3a609cec9c89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                 processed_summaries  \\\n",
            "0  stereo matching is one of the widely used tech...   \n",
            "1  the recent advancements in artificial intellig...   \n",
            "2  in this paper we proposed a novel mutual consi...   \n",
            "3  consistency training has proven to be an advan...   \n",
            "4  to ensure safety in automated driving the corr...   \n",
            "\n",
            "                                 tokenized_summaries  \n",
            "0  [stereo, matching, is, one, of, the, widely, u...  \n",
            "1  [the, recent, advancements, in, artificial, in...  \n",
            "2  [in, this, paper, we, proposed, a, novel, mutu...  \n",
            "3  [consistency, training, has, proven, to, be, a...  \n",
            "4  [to, ensure, safety, in, automated, driving, t...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stop Word Removal**"
      ],
      "metadata": {
        "id": "Hf8OEm-3NNKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stopwords(tokens):\n",
        "    return [word for word in tokens if word not in stop_words]\n",
        "\n",
        "df['filtered_summaries'] = df['tokenized_summaries'].apply(remove_stopwords)\n",
        "print(df[['tokenized_summaries', 'filtered_summaries']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eT-SNUSTNQE1",
        "outputId": "c45ff464-9471-4019-9996-56275990a398"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                 tokenized_summaries  \\\n",
            "0  [stereo, matching, is, one, of, the, widely, u...   \n",
            "1  [the, recent, advancements, in, artificial, in...   \n",
            "2  [in, this, paper, we, proposed, a, novel, mutu...   \n",
            "3  [consistency, training, has, proven, to, be, a...   \n",
            "4  [to, ensure, safety, in, automated, driving, t...   \n",
            "\n",
            "                                  filtered_summaries  \n",
            "0  [stereo, matching, one, widely, used, techniqu...  \n",
            "1  [recent, advancements, artificial, intelligenc...  \n",
            "2  [paper, proposed, novel, mutual, consistency, ...  \n",
            "3  [consistency, training, proven, advanced, semi...  \n",
            "4  [ensure, safety, automated, driving, correct, ...  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lemmatization**"
      ],
      "metadata": {
        "id": "xzGl-xvaNZ7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_tokens(tokens):\n",
        "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "df['lemmatized_summaries'] = df['filtered_summaries'].apply(lemmatize_tokens)\n",
        "print(df[['filtered_summaries', 'lemmatized_summaries']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wHEuk5yNdwl",
        "outputId": "cc079577-b13f-4ac1-b05d-87e3292be001"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                  filtered_summaries  \\\n",
            "0  [stereo, matching, one, widely, used, techniqu...   \n",
            "1  [recent, advancements, artificial, intelligenc...   \n",
            "2  [paper, proposed, novel, mutual, consistency, ...   \n",
            "3  [consistency, training, proven, advanced, semi...   \n",
            "4  [ensure, safety, automated, driving, correct, ...   \n",
            "\n",
            "                                lemmatized_summaries  \n",
            "0  [stereo, matching, one, widely, used, techniqu...  \n",
            "1  [recent, advancement, artificial, intelligence...  \n",
            "2  [paper, proposed, novel, mutual, consistency, ...  \n",
            "3  [consistency, training, proven, advanced, semi...  \n",
            "4  [ensure, safety, automated, driving, correct, ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Rejoining**"
      ],
      "metadata": {
        "id": "xlBixS-5NyOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['clean_summaries'] = df['lemmatized_summaries'].apply(lambda x: ' '.join(x))\n",
        "print(df[['summaries', 'clean_summaries']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIG6E_RiN4Fy",
        "outputId": "bc2e88ec-4045-4cf2-f178-6b6819f301f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                           summaries  \\\n",
            "0  Stereo matching is one of the widely used tech...   \n",
            "1  The recent advancements in artificial intellig...   \n",
            "2  In this paper, we proposed a novel mutual cons...   \n",
            "3  Consistency training has proven to be an advan...   \n",
            "4  To ensure safety in automated driving, the cor...   \n",
            "\n",
            "                                     clean_summaries  \n",
            "0  stereo matching one widely used technique infe...  \n",
            "1  recent advancement artificial intelligence ai ...  \n",
            "2  paper proposed novel mutual consistency networ...  \n",
            "3  consistency training proven advanced semisuper...  \n",
            "4  ensure safety automated driving correct percep...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PoS Tagging**"
      ],
      "metadata": {
        "id": "juoVAkF7N-Bj"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b16a3e2",
        "outputId": "1d4f5bdc-dbf6-4f11-b73a-1c85255742b8"
      },
      "source": [
        "import nltk\n",
        "from nltk import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "df['pos_tagged_summaries'] = df['lemmatized_summaries'].apply(pos_tag)\n",
        "print(df[['lemmatized_summaries', 'pos_tagged_summaries']].head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                lemmatized_summaries  \\\n",
            "0  [stereo, matching, one, widely, used, techniqu...   \n",
            "1  [recent, advancement, artificial, intelligence...   \n",
            "2  [paper, proposed, novel, mutual, consistency, ...   \n",
            "3  [consistency, training, proven, advanced, semi...   \n",
            "4  [ensure, safety, automated, driving, correct, ...   \n",
            "\n",
            "                                pos_tagged_summaries  \n",
            "0  [(stereo, NN), (matching, VBG), (one, CD), (wi...  \n",
            "1  [(recent, JJ), (advancement, JJ), (artificial,...  \n",
            "2  [(paper, NN), (proposed, VBN), (novel, JJ), (m...  \n",
            "3  [(consistency, NN), (training, VBG), (proven, ...  \n",
            "4  [(ensure, VB), (safety, NN), (automated, VBN),...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Noun Phrase Frequency**"
      ],
      "metadata": {
        "id": "P6YLuatgou2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.chunk import RegexpParser\n",
        "\n",
        "grammar = r\"\"\"\n",
        "  NP: {<DT>?<JJ>*<NN.*>+}\n",
        "\"\"\"\n",
        "\n",
        "np_parser = RegexpParser(grammar)\n",
        "\n",
        "df['chunked_summaries'] = df['pos_tagged_summaries'].apply(np_parser.parse)\n",
        "print(df[['pos_tagged_summaries', 'chunked_summaries']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQbRIeXMoyfR",
        "outputId": "0ca4b17b-5885-482e-b764-dd44ccbdb5da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                pos_tagged_summaries  \\\n",
            "0  [(stereo, NN), (matching, VBG), (one, CD), (wi...   \n",
            "1  [(recent, JJ), (advancement, JJ), (artificial,...   \n",
            "2  [(paper, NN), (proposed, VBN), (novel, JJ), (m...   \n",
            "3  [(consistency, NN), (training, VBG), (proven, ...   \n",
            "4  [(ensure, VB), (safety, NN), (automated, VBN),...   \n",
            "\n",
            "                                   chunked_summaries  \n",
            "0  [[(stereo, NN)], (matching, VBG), (one, CD), (...  \n",
            "1  [[(recent, JJ), (advancement, JJ), (artificial...  \n",
            "2  [[(paper, NN)], (proposed, VBN), [(novel, JJ),...  \n",
            "3  [[(consistency, NN)], (training, VBG), (proven...  \n",
            "4  [(ensure, VB), [(safety, NN)], (automated, VBN...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def extract_nps(tree):\n",
        "    noun_phrases = []\n",
        "    for subtree in tree:\n",
        "        if isinstance(subtree, nltk.tree.Tree) and subtree.label() == 'NP':\n",
        "            np_words = [word for word, tag in subtree.leaves()]\n",
        "            noun_phrases.append(' '.join(np_words))\n",
        "    return noun_phrases\n",
        "\n",
        "df['extracted_nps'] = df['chunked_summaries'].apply(extract_nps)\n",
        "all_noun_phrases = [np for nps_list in df['extracted_nps'] for np in nps_list]\n",
        "np_counts = Counter(all_noun_phrases)\n",
        "print(\"\\nTop 20 Most Frequent Noun Phrases:\")\n",
        "print(np_counts.most_common(20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6pWE6wdyQU0",
        "outputId": "629ad5ab-a359-4746-e4dc-97233a9bb516"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 20 Most Frequent Noun Phrases:\n",
            "[('method', 272), ('image', 206), ('model', 187), ('approach', 133), ('data', 131), ('medical image segmentation', 121), ('image segmentation', 114), ('segmentation', 110), ('semantic segmentation', 103), ('performance', 91), ('datasets', 86), ('task', 82), ('paper', 80), ('network', 79), ('medical image', 72), ('semantic image segmentation', 67), ('result', 63), ('deep learning', 63), ('framework', 62), ('neural network', 56)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NLTK Pipeline Function**"
      ],
      "metadata": {
        "id": "QI1v_pXk1q-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def nltk_preprocessing_pipeline(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "\n",
        "    text = text.lower()\n",
        "\n",
        "    emoji_pattern = re.compile(\n",
        "        \"[\"\n",
        "        \"\\U0001F600-\\U0001F64F\"\n",
        "        \"\\U0001F300-\\U0001F5FF\"\n",
        "        \"\\U0001F680-\\U0001F6FF\"\n",
        "        \"\\U0001F1E0-\\U0001F1FF\"\n",
        "        \"\\U00002702-\\U000027B0\"\n",
        "        \"\\U000024C2-\\U0001F251\"\n",
        "        \"]+\", flags=re.UNICODE\n",
        "    )\n",
        "    text = emoji_pattern.sub(r'', text)\n",
        "\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    tokenized_words = word_tokenize(text)\n",
        "    filtered_words = [word for word in tokenized_words if word not in stop_words]\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
        "    clean_summary = ' '.join(lemmatized_words)\n",
        "\n",
        "    return clean_summary\n",
        "\n",
        "print(\"NLTK preprocessing pipeline function created successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H04HwS9M1uAd",
        "outputId": "2f89c0b5-b5b2-4ded-cf37-a458a562e1fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK preprocessing pipeline function created successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['clean_summaries_pipeline'] = df['summaries'].apply(nltk_preprocessing_pipeline)\n",
        "print(\"\\nComparison of previous clean_summaries and new clean_summaries_pipeline (first 5 rows):\")\n",
        "print(df[['clean_summaries', 'clean_summaries_pipeline']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcliYxC72tPU",
        "outputId": "eee6b203-d24e-4344-f55d-e446df7ec8c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Comparison of previous clean_summaries and new clean_summaries_pipeline (first 5 rows):\n",
            "                                     clean_summaries  \\\n",
            "0  stereo matching one widely used technique infe...   \n",
            "1  recent advancement artificial intelligence ai ...   \n",
            "2  paper proposed novel mutual consistency networ...   \n",
            "3  consistency training proven advanced semisuper...   \n",
            "4  ensure safety automated driving correct percep...   \n",
            "\n",
            "                            clean_summaries_pipeline  \n",
            "0  stereo matching one widely used technique infe...  \n",
            "1  recent advancement artificial intelligence ai ...  \n",
            "2  paper proposed novel mutual consistency networ...  \n",
            "3  consistency training proven advanced semisuper...  \n",
            "4  ensure safety automated driving correct percep...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Using spaCy**"
      ],
      "metadata": {
        "id": "PYE7aJeN3ST8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loading the dataset**"
      ],
      "metadata": {
        "id": "4ue_4WiT3dFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/arxiv_data.csv\", engine='python', nrows=1000)\n",
        "display(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "uMCVPhCC3fva",
        "outputId": "32efe8bb-c8a9-490d-8b16-828e8ece8ef3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                              titles  \\\n",
              "0  Survey on Semantic Stereo Matching / Semantic ...   \n",
              "1  FUTURE-AI: Guiding Principles and Consensus Re...   \n",
              "2  Enforcing Mutual Consistency of Hard Regions f...   \n",
              "3  Parameter Decoupling Strategy for Semi-supervi...   \n",
              "4  Background-Foreground Segmentation for Interio...   \n",
              "\n",
              "                                           summaries  \\\n",
              "0  Stereo matching is one of the widely used tech...   \n",
              "1  The recent advancements in artificial intellig...   \n",
              "2  In this paper, we proposed a novel mutual cons...   \n",
              "3  Consistency training has proven to be an advan...   \n",
              "4  To ensure safety in automated driving, the cor...   \n",
              "\n",
              "                         terms  \n",
              "0           ['cs.CV', 'cs.LG']  \n",
              "1  ['cs.CV', 'cs.AI', 'cs.LG']  \n",
              "2           ['cs.CV', 'cs.AI']  \n",
              "3                    ['cs.CV']  \n",
              "4           ['cs.CV', 'cs.LG']  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ee32eb86-1d63-4b56-ad73-4cca24bc1705\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>titles</th>\n",
              "      <th>summaries</th>\n",
              "      <th>terms</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Survey on Semantic Stereo Matching / Semantic ...</td>\n",
              "      <td>Stereo matching is one of the widely used tech...</td>\n",
              "      <td>['cs.CV', 'cs.LG']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>FUTURE-AI: Guiding Principles and Consensus Re...</td>\n",
              "      <td>The recent advancements in artificial intellig...</td>\n",
              "      <td>['cs.CV', 'cs.AI', 'cs.LG']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Enforcing Mutual Consistency of Hard Regions f...</td>\n",
              "      <td>In this paper, we proposed a novel mutual cons...</td>\n",
              "      <td>['cs.CV', 'cs.AI']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Parameter Decoupling Strategy for Semi-supervi...</td>\n",
              "      <td>Consistency training has proven to be an advan...</td>\n",
              "      <td>['cs.CV']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Background-Foreground Segmentation for Interio...</td>\n",
              "      <td>To ensure safety in automated driving, the cor...</td>\n",
              "      <td>['cs.CV', 'cs.LG']</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ee32eb86-1d63-4b56-ad73-4cca24bc1705')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ee32eb86-1d63-4b56-ad73-4cca24bc1705 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ee32eb86-1d63-4b56-ad73-4cca24bc1705');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"titles\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"FUTURE-AI: Guiding Principles and Consensus Recommendations for Trustworthy Artificial Intelligence in Future Medical Imaging\",\n          \"Background-Foreground Segmentation for Interior Sensing in Automotive Industry\",\n          \"Enforcing Mutual Consistency of Hard Regions for Semi-supervised Medical Image Segmentation\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summaries\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"The recent advancements in artificial intelligence (AI) combined with the\\nextensive amount of data generated by today's clinical systems, has led to the\\ndevelopment of imaging AI solutions across the whole value chain of medical\\nimaging, including image reconstruction, medical image segmentation,\\nimage-based diagnosis and treatment planning. Notwithstanding the successes and\\nfuture potential of AI in medical imaging, many stakeholders are concerned of\\nthe potential risks and ethical implications of imaging AI solutions, which are\\nperceived as complex, opaque, and difficult to comprehend, utilise, and trust\\nin critical clinical applications. Despite these concerns and risks, there are\\ncurrently no concrete guidelines and best practices for guiding future AI\\ndevelopments in medical imaging towards increased trust, safety and adoption.\\nTo bridge this gap, this paper introduces a careful selection of guiding\\nprinciples drawn from the accumulated experiences, consensus, and best\\npractices from five large European projects on AI in Health Imaging. These\\nguiding principles are named FUTURE-AI and its building blocks consist of (i)\\nFairness, (ii) Universality, (iii) Traceability, (iv) Usability, (v) Robustness\\nand (vi) Explainability. In a step-by-step approach, these guidelines are\\nfurther translated into a framework of concrete recommendations for specifying,\\ndeveloping, evaluating, and deploying technically, clinically and ethically\\ntrustworthy AI solutions into clinical practice.\",\n          \"To ensure safety in automated driving, the correct perception of the\\nsituation inside the car is as important as its environment. Thus, seat\\noccupancy detection and classification of detected instances play an important\\nrole in interior sensing. By the knowledge of the seat occupancy status, it is\\npossible to, e.g., automate the airbag deployment control. Furthermore, the\\npresence of a driver, which is necessary for partially automated driving cars\\nat the automation levels two to four can be verified. In this work, we compare\\ndifferent statistical methods from the field of image segmentation to approach\\nthe problem of background-foreground segmentation in camera based interior\\nsensing. In the recent years, several methods based on different techniques\\nhave been developed and applied to images or videos from different\\napplications. The peculiarity of the given scenarios of interior sensing is,\\nthat the foreground instances and the background both contain static as well as\\ndynamic elements. In data considered in this work, even the camera position is\\nnot completely fixed. We review and benchmark three different methods ranging,\\ni.e., Gaussian Mixture Models (GMM), Morphological Snakes and a deep neural\\nnetwork, namely a Mask R-CNN. In particular, the limitations of the classical\\nmethods, GMM and Morphological Snakes, for interior sensing are shown.\\nFurthermore, it turns, that it is possible to overcome these limitations by\\ndeep learning, e.g.\\\\ using a Mask R-CNN. Although only a small amount of ground\\ntruth data was available for training, we enabled the Mask R-CNN to produce\\nhigh quality background-foreground masks via transfer learning. Moreover, we\\ndemonstrate that certain augmentation as well as pre- and post-processing\\nmethods further enhance the performance of the investigated methods.\",\n          \"In this paper, we proposed a novel mutual consistency network (MC-Net+) to\\neffectively exploit the unlabeled hard regions for semi-supervised medical\\nimage segmentation. The MC-Net+ model is motivated by the observation that deep\\nmodels trained with limited annotations are prone to output highly uncertain\\nand easily mis-classified predictions in the ambiguous regions (e.g. adhesive\\nedges or thin branches) for the image segmentation task. Leveraging these\\nregion-level challenging samples can make the semi-supervised segmentation\\nmodel training more effective. Therefore, our proposed MC-Net+ model consists\\nof two new designs. First, the model contains one shared encoder and multiple\\nsightly different decoders (i.e. using different up-sampling strategies). The\\nstatistical discrepancy of multiple decoders' outputs is computed to denote the\\nmodel's uncertainty, which indicates the unlabeled hard regions. Second, a new\\nmutual consistency constraint is enforced between one decoder's probability\\noutput and other decoders' soft pseudo labels. In this way, we minimize the\\nmodel's uncertainty during training and force the model to generate invariant\\nand low-entropy results in such challenging areas of unlabeled data, in order\\nto learn a generalized feature representation. We compared the segmentation\\nresults of the MC-Net+ with five state-of-the-art semi-supervised approaches on\\nthree public medical datasets. Extension experiments with two common\\nsemi-supervised settings demonstrate the superior performance of our model over\\nother existing methods, which sets a new state of the art for semi-supervised\\nmedical image segmentation.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"terms\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"['cs.CV', 'cs.AI', 'cs.LG']\",\n          \"['cs.CV']\",\n          \"['cs.CV', 'cs.LG']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cleaning the data**"
      ],
      "metadata": {
        "id": "Lqjey5Bf3qoS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "\n",
        "    text = text.lower()\n",
        "\n",
        "    emoji_pattern = re.compile(\n",
        "        \"[\"\n",
        "        \"\\U0001F600-\\U0001F64F\"\n",
        "        \"\\U0001F300-\\U0001F5FF\"\n",
        "        \"\\U0001F680-\\U0001F6FF\"\n",
        "        \"\\U0001F1E0-\\U0001F1FF\"\n",
        "        \"\\U00002702-\\U000027B0\"\n",
        "        \"\\U000024C2-\\U0001F251\"\n",
        "        \"]+\", flags=re.UNICODE\n",
        "    )\n",
        "    text = emoji_pattern.sub(r'', text)\n",
        "\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text"
      ],
      "metadata": {
        "id": "P9zVhjOg3u7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['processed_summaries'] = df['summaries'].apply(preprocess_text)\n",
        "print(df[['summaries', 'processed_summaries']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UOgPNdW3649",
        "outputId": "cd73419d-95eb-4e19-dc76-7015cccf5a96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                           summaries  \\\n",
            "0  Stereo matching is one of the widely used tech...   \n",
            "1  The recent advancements in artificial intellig...   \n",
            "2  In this paper, we proposed a novel mutual cons...   \n",
            "3  Consistency training has proven to be an advan...   \n",
            "4  To ensure safety in automated driving, the cor...   \n",
            "\n",
            "                                 processed_summaries  \n",
            "0  stereo matching is one of the widely used tech...  \n",
            "1  the recent advancements in artificial intellig...  \n",
            "2  in this paper we proposed a novel mutual consi...  \n",
            "3  consistency training has proven to be an advan...  \n",
            "4  to ensure safety in automated driving the corr...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Word Tokenization**"
      ],
      "metadata": {
        "id": "c2dDrGsB4AWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def tokenize_text_spacy(text):\n",
        "    doc = nlp(text)\n",
        "    return [token.text for token in doc]\n",
        "\n",
        "df['tokenized_summaries'] = df['processed_summaries'].apply(tokenize_text_spacy)\n",
        "\n",
        "print(df[['processed_summaries', 'tokenized_summaries']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLU2ydnL4EGz",
        "outputId": "46af9f9a-10c9-4f0e-dcd4-7a55a61dc12c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                 processed_summaries  \\\n",
            "0  stereo matching is one of the widely used tech...   \n",
            "1  the recent advancements in artificial intellig...   \n",
            "2  in this paper we proposed a novel mutual consi...   \n",
            "3  consistency training has proven to be an advan...   \n",
            "4  to ensure safety in automated driving the corr...   \n",
            "\n",
            "                                 tokenized_summaries  \n",
            "0  [stereo, matching, is, one, of, the, widely, u...  \n",
            "1  [the, recent, advancements, in, artificial, in...  \n",
            "2  [in, this, paper, we, proposed, a, novel, mutu...  \n",
            "3  [consistency, training, has, proven, to, be, a...  \n",
            "4  [to, ensure, safety, in, automated, driving, t...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stop Word Removal**"
      ],
      "metadata": {
        "id": "NBbulfxv4TP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords_spacy(tokens):\n",
        "    return [token.text for token in nlp(' '.join(tokens)) if not token.is_stop]\n",
        "\n",
        "df['summaries_no_stopwords'] = df['tokenized_summaries'].apply(remove_stopwords_spacy)\n",
        "print(df[['tokenized_summaries', 'summaries_no_stopwords']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YehFgYMU4Wzw",
        "outputId": "e2568e09-62c5-4855-8afb-c796d5a731f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                 tokenized_summaries  \\\n",
            "0  [stereo, matching, is, one, of, the, widely, u...   \n",
            "1  [the, recent, advancements, in, artificial, in...   \n",
            "2  [in, this, paper, we, proposed, a, novel, mutu...   \n",
            "3  [consistency, training, has, proven, to, be, a...   \n",
            "4  [to, ensure, safety, in, automated, driving, t...   \n",
            "\n",
            "                              summaries_no_stopwords  \n",
            "0  [stereo, matching, widely, techniques, inferri...  \n",
            "1  [recent, advancements, artificial, intelligenc...  \n",
            "2  [paper, proposed, novel, mutual, consistency, ...  \n",
            "3  [consistency, training, proven, advanced, semi...  \n",
            "4  [ensure, safety, automated, driving, correct, ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lemmatization**"
      ],
      "metadata": {
        "id": "0hycXoZq4kSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize_text_spacy(tokens):\n",
        "    doc = nlp(' '.join(tokens))\n",
        "    return [token.lemma_ for token in doc]\n",
        "\n",
        "df['lemmatized_summaries'] = df['summaries_no_stopwords'].apply(lemmatize_text_spacy)\n",
        "print(df[['summaries_no_stopwords', 'lemmatized_summaries']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_RpCoGK4ssp",
        "outputId": "0ad8a30f-8e6e-46b6-fc27-4a6912762769"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                              summaries_no_stopwords  \\\n",
            "0  [stereo, matching, widely, techniques, inferri...   \n",
            "1  [recent, advancements, artificial, intelligenc...   \n",
            "2  [paper, proposed, novel, mutual, consistency, ...   \n",
            "3  [consistency, training, proven, advanced, semi...   \n",
            "4  [ensure, safety, automated, driving, correct, ...   \n",
            "\n",
            "                                lemmatized_summaries  \n",
            "0  [stereo, matching, widely, technique, infer, d...  \n",
            "1  [recent, advancement, artificial, intelligence...  \n",
            "2  [paper, propose, novel, mutual, consistency, n...  \n",
            "3  [consistency, training, prove, advanced, semis...  \n",
            "4  [ensure, safety, automate, drive, correct, per...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Rejoining**"
      ],
      "metadata": {
        "id": "m7cCNaKx417X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rejoin_summaries(lemmas):\n",
        "    return ' '.join(lemmas)\n",
        "\n",
        "df['final_summaries'] = df['lemmatized_summaries'].apply(rejoin_summaries)\n",
        "print(df[['lemmatized_summaries', 'final_summaries']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JFbG73k47me",
        "outputId": "6f3c0a43-5ae6-4b7b-91c3-f9838d52203a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                lemmatized_summaries  \\\n",
            "0  [stereo, matching, widely, technique, infer, d...   \n",
            "1  [recent, advancement, artificial, intelligence...   \n",
            "2  [paper, propose, novel, mutual, consistency, n...   \n",
            "3  [consistency, training, prove, advanced, semis...   \n",
            "4  [ensure, safety, automate, drive, correct, per...   \n",
            "\n",
            "                                     final_summaries  \n",
            "0  stereo matching widely technique infer depth s...  \n",
            "1  recent advancement artificial intelligence ai ...  \n",
            "2  paper propose novel mutual consistency network...  \n",
            "3  consistency training prove advanced semisuperv...  \n",
            "4  ensure safety automate drive correct perceptio...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Noun Phrase Frequencies**"
      ],
      "metadata": {
        "id": "JAsM4i6V4-n4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_noun_phrases(text):\n",
        "    doc = nlp(text)\n",
        "    return [chunk.lemma_ for chunk in doc.noun_chunks]\n",
        "\n",
        "df['noun_phrases'] = df['final_summaries'].apply(extract_noun_phrases)\n",
        "print(df[['final_summaries', 'noun_phrases']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEtMeC8M5N1M",
        "outputId": "b2b6c185-26d4-4d66-9507-a4fd4bde13c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                     final_summaries  \\\n",
            "0  stereo matching widely technique infer depth s...   \n",
            "1  recent advancement artificial intelligence ai ...   \n",
            "2  paper propose novel mutual consistency network...   \n",
            "3  consistency training prove advanced semisuperv...   \n",
            "4  ensure safety automate drive correct perceptio...   \n",
            "\n",
            "                                        noun_phrases  \n",
            "0  [stereo matching, widely technique infer depth...  \n",
            "1  [recent advancement artificial intelligence, e...  \n",
            "2  [paper, novel mutual consistency network mcnet...  \n",
            "3  [consistency training, advanced semisupervise ...  \n",
            "4  [safety automate drive correct perception situ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "all_noun_phrases = []\n",
        "for phrases_list in df['noun_phrases']:\n",
        "    all_noun_phrases.extend(phrases_list)\n",
        "\n",
        "noun_phrase_counts = Counter(all_noun_phrases)\n",
        "\n",
        "print(\"Top 10 most frequent noun phrases:\")\n",
        "for noun_phrase, count in noun_phrase_counts.most_common(10):\n",
        "    print(f\"'{noun_phrase}': {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Izl0IIV95Z8g",
        "outputId": "1d2d3b79-90c8-4efb-d7a7-d151c7bf6664"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 most frequent noun phrases:\n",
            "'method': 55\n",
            "'algorithm': 51\n",
            "'deep learning': 38\n",
            "'paper': 37\n",
            "'propose method': 37\n",
            "'deep neural network': 30\n",
            "'image segmentation': 30\n",
            "'image': 27\n",
            "'approach': 26\n",
            "'performance': 25\n"
          ]
        }
      ]
    }
  ]
}